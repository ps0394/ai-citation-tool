name: AI Citation Benchmark

on:
  schedule:
    # Run daily at 9 AM UTC
    - cron: '0 9 * * *'
  workflow_dispatch:
    inputs:
      dry_run:
        description: 'Run in dry-run mode'
        required: false
        default: 'false'
        type: choice
        options:
        - 'true'
        - 'false'

permissions:
  contents: write  # Allow workflow to write to repository

jobs:
  benchmark:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        
    - name: Run benchmark
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        PERPLEXITY_API_KEY: ${{ secrets.PERPLEXITY_API_KEY }}
      run: |
        if [ "${{ github.event.inputs.dry_run }}" = "true" ]; then
          python run_benchmark.py --dry-run
        else
          python run_benchmark.py
        fi
        
    - name: Archive results
      run: |
        mkdir -p docs/data
        cp results_*.csv docs/data/ || echo "No results file found"
        
    - name: Commit and push results
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add docs/data/
        git diff --staged --quiet || git commit -m "Update benchmark results $(date)"
        git push